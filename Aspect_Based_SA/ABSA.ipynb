{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Embedding\n",
    "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D, LSTM, Dropout\n",
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "\n",
    "from matplotlib import pyplot\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter=PorterStemmer()\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]','',text, re.UNICODE)\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "#     tokens = [word for word in tokens if word not in stop_words]\n",
    "    tokens = [porter.stem(word) for word in tokens]\n",
    "    tokens = \" \".join(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_corpus():\n",
    "\n",
    "    train_pos = open('Data/train-pos.txt','r',encoding=\"utf8\").read().split('\\n')[0:12500]\n",
    "    train_neg = open('Data/train-neg.txt','r',encoding=\"utf8\").read().split('\\n')[0:12500]\n",
    "    test_pos = open('Data/test-pos.txt','r',encoding=\"utf8\").read().split('\\n')[0:12500]\n",
    "    test_neg = open('Data/test-neg.txt','r',encoding=\"utf8\").read().split('\\n')[0:12500]\n",
    "    \n",
    "    train_pos = [clean_text(line) for line in train_pos]\n",
    "    train_neg = [clean_text(line) for line in train_neg]\n",
    "    test_pos = [clean_text(line) for line in test_pos]\n",
    "    test_neg = [clean_text(line) for line in test_neg]\n",
    "    \n",
    "    return train_pos, train_neg, test_pos, test_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data1(top_words):\n",
    "    vocab = {}\n",
    "    train_neg = open('Data/sent_neg.txt',encoding=\"utf8\").read().split('\\n')\n",
    "    train_pos = open('Data/sent_pos.txt',encoding=\"utf8\").read().split('\\n')\n",
    "    \n",
    "    train_neg = [clean_text(line) for line in train_neg]\n",
    "    train_pos = [clean_text(line) for line in train_pos]\n",
    "    \n",
    "    train = train_neg + train_pos\n",
    "    \n",
    "    for sent in train:\n",
    "        for word in sent.split():\n",
    "            if word in vocab:\n",
    "                vocab[word]+=1\n",
    "            else:\n",
    "                vocab[word]=1\n",
    "                    \n",
    "    vocab_arr = sorted(vocab.items(), key=lambda x:x[1], reverse=True)\n",
    "    vocab_arr = [x[0] for x in vocab_arr]\n",
    "    top_vocab = {word:index for index,word in enumerate(vocab_arr[0:top_words])}  # index 0 reserved for padding and OOV words\n",
    "    \n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    \n",
    "    for index,sent in enumerate(train_neg):\n",
    "        t = []\n",
    "        for word in sent.split():\n",
    "            if word in top_vocab:\n",
    "                t.append(top_vocab[word])\n",
    "            else:\n",
    "                t.append(0)\n",
    "                \n",
    "        x_train.append(t)\n",
    "        y_train.append(0)\n",
    "        \n",
    "    for index,sent in enumerate(train_pos):\n",
    "        t = []\n",
    "        for word in sent.split():\n",
    "            if word in top_vocab:\n",
    "                t.append(top_vocab[word])\n",
    "            else:\n",
    "                t.append(0)\n",
    "                \n",
    "        x_train.append(t)\n",
    "        y_train.append(1)\n",
    "        \n",
    "        \n",
    "    return top_vocab, x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data2(top_words):                                                           # for deep learning\n",
    "    vocab = {}\n",
    "    \n",
    "    train = train_pos + train_neg\n",
    "    test = test_pos + test_neg\n",
    "    \n",
    "    for sent in train:\n",
    "        for word in sent.split():\n",
    "            if word in vocab:\n",
    "                vocab[word]+=1\n",
    "            else:\n",
    "                vocab[word]=1\n",
    "    vocab_arr = sorted(vocab.items(), key=lambda x:x[1], reverse=True)\n",
    "    vocab_arr = [x[0] for x in vocab_arr]\n",
    "    top_vocab = {word:index for index,word in enumerate(vocab_arr[0:top_words])}  # index 0 reserved for padding and OOV words\n",
    "    \n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    \n",
    "    x_test = []\n",
    "    y_test = []\n",
    "    \n",
    "    for index,sent in enumerate(train):\n",
    "        t = []\n",
    "        for word in sent.split():\n",
    "            if word in top_vocab:\n",
    "                t.append(top_vocab[word])\n",
    "            else:\n",
    "                t.append(0)\n",
    "                \n",
    "        x_train.append(t)\n",
    "        \n",
    "        if index<12500:\n",
    "            y_train.append(1)\n",
    "        else:\n",
    "            y_train.append(0)\n",
    "\n",
    "    for index,sent in enumerate(test):\n",
    "        t = []\n",
    "        for word in sent.split():\n",
    "            if word in top_vocab:\n",
    "                t.append(top_vocab[word])\n",
    "            else:\n",
    "                t.append(0)\n",
    "                \n",
    "        x_test.append(t)\n",
    "        \n",
    "        if index<12500:\n",
    "            y_test.append(1)\n",
    "        else:\n",
    "            y_test.append(0)\n",
    "            \n",
    "    return top_vocab, (x_train,y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padSequences(arr, max_words):        \n",
    "    ans = [x[0:max_words] if len(x)>=max_words else [x[i] if i<len(x) else 0 for i in range(max_words)] for x in arr]\n",
    "    return ans\n",
    "\n",
    "def shuffle(x, y):\n",
    "    p = np.random.permutation(len(x))\n",
    "    return x[p],y[p]\n",
    "\n",
    "def give_phrases(sent):\n",
    "    sent = sent.replace(',','.')\n",
    "    sent = sent.replace('and','.')\n",
    "    sent = sent.replace('but','.')\n",
    "    \n",
    "    return sent.split('.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sentences(model,word, movie_no, df):\n",
    "    \n",
    "    reviews = df.iloc[:,3].values\n",
    "    movie_nos = df.iloc[:,1].values\n",
    "    \n",
    "    phrases = []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        if movie_nos[i]==movie_no:\n",
    "            phrases+=give_phrases(reviews[i])\n",
    "            \n",
    "    pos_sen = []\n",
    "    neg_sen = []\n",
    "\n",
    "   \n",
    "    for phrase in phrases:\n",
    "        if word in phrase:\n",
    "            sen = sentiment(model,phrase)\n",
    "            if sen == 1:\n",
    "                pos_sen.append(phrase)\n",
    "            else:\n",
    "                neg_sen.append(phrase)\n",
    "              \n",
    "    return pos_sen, neg_sen\n",
    "\n",
    "def sentiment(model, sent):\n",
    "    sent = clean_text(sent)\n",
    "    \n",
    "    vec = [top_vocab[w] if w in top_vocab else 0 for w in sent.split()]\n",
    "    vec  = padSequences([vec], max_words)[0]\n",
    "    \n",
    "    pred = model.predict(np.array([vec]))[0][0]\n",
    "    \n",
    "    if pred>=0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos, train_neg, test_pos, test_neg = clean_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 500\n",
    "vocab_size = 10000\n",
    "embedding_dim = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_vocab, (x_train, y_train), (x_test, y_test) = prepare_data2(vocab_size)\n",
    "x_train = padSequences(x_train, max_words)\n",
    "x_test = padSequences(x_test, max_words)\n",
    "x_train,y_train,x_test,y_test = np.array(x_train), np.array(y_train), np.array(x_test), np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training CNN Model\")                                                \n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=max_words))\n",
    "\n",
    "model.add(Conv1D(filters=32, kernel_size=8, padding='same', activation='relu')) \n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(Flatten())                                                                                            #keras model for CNN\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=2, batch_size=128, verbose=1)\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "cnnModel = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
